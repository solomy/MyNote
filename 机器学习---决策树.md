# 机器学习---决策树

本文尽量采用最简洁最直白的描述，对于有些解释有出入的恳请指出。

前言：经过两次算法的面试，发现工业界对于机器学习主要是在分布式和效率上，对于精度的取舍可能不是很看重。因此，集成学习和深度学习是要掌握透彻的。（XGboost、GBDT和RF几乎必问）

## 什么是决策树？

决策树是一种采用属性(特征)划分来解决分类问题的算法，通常有3个步骤：

1. 特征选择：对于多维特征，依次计算不同维度上特征的信息增益。
2. 决策树生成：选取最大信息增益的特征作为根节点，依次生成子节点。
3. 剪枝：对抗过拟合，去除分支。（预剪枝，后剪枝）

3种决策树(划分)算法

1. ID3：采用**信息增益**，最早的决策树算法
2. C4.5：采用**信息增益比**
3. CART(Classification and regression Tree)：采用**基尼系数**

## 优缺点

| 优点                           | 缺点                                                        |
| ------------------------------ | ----------------------------------------------------------- |
| 运行速度快，能够应对大型数据源 | 易过拟合                                                    |
| 适合有缺失属性的样本           | 忽略特征之间的关联                                          |
| 可以同时处理标称型与数值型数据 | 类别样本不均衡时，ID3偏好数目多的属性，CART偏好数目少的属性 |
| 可视化                         |                                                             |

## 连续和缺失值

1. **连续值：**类型属性离散化，如二分法
2. **缺失值：**

- 如何属性缺失情况下划分？（给定样本权重，权重等价于属性缺失占的比例）
- 给定划分后属性后，样本的属性缺失如何划分？（让同一个样本以不同概率分到不同子节点）