# 机器学习---RF、GBDT、XGBoost

前言：RF、GBDT、XGBoost都是集成学习，即组合多个基学习器预测结果来得到最终的结果，提高了模型的泛化性和鲁棒性。分为两类：

1. 基学习器之间强依赖，必须串行生成：Boosting
2. 基学习器之前弱依赖，可并行生成：Bagging、RF

## Bagging与随机森林

原理：相互交替的采用子集训练不同的学习器。主要关注降低方差，在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。

Bagging：

1. 放回抽样
2. 多数表决（分类）或简单平均（回归）

RF：

1. 随机选择样本（放回抽样）
2. **随机选择特征**
3. 构建决策树
4. 随机森林投票（平均）。

## Boosting

原理：不断迭代，调整分布。主要关注降低偏差，提高泛化。

1. 初学习器
2. 调整分布，使得错误的样本有更高的关注（加权）
3. 基于调制后的分布，循环训练下一个基训练器，直到T次循环结束。（生成新学习器）

## 结合策略

平均法：简单平均、加权平均

投票法：绝对多数投票、相对多数投票、加权投票

学习法：通过元学习器结合

## GBDT

GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，在**残差减小的梯度方向**上建立模型。

所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting中关注正确错误的样本加权有着很大的区别。
GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树。

## XGBoost

**XGBoost利用并行的CPU解决迭代次数过多问题**

#### 与GBDT区别

1. **基学习器**：GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）；
2. **代价函数求导**：传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；
3. **正则化**：XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单
4. **缺失值的处理**：XGBoost还可以自动学习出它的分裂方向
5. **并行**：多线程进行各个特征的增益计算（非Tree粒度的并行）